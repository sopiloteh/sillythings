{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1043970,"sourceType":"datasetVersion","datasetId":576697},{"sourceId":1435584,"sourceType":"datasetVersion","datasetId":841044},{"sourceId":6468000,"sourceType":"datasetVersion","datasetId":3735130},{"sourceId":6548675,"sourceType":"datasetVersion","datasetId":3784992},{"sourceId":6651716,"sourceType":"datasetVersion","datasetId":3839085},{"sourceId":6778410,"sourceType":"datasetVersion","datasetId":3900971},{"sourceId":6858403,"sourceType":"datasetVersion","datasetId":3941948},{"sourceId":7293307,"sourceType":"datasetVersion","datasetId":4230094},{"sourceId":7293456,"sourceType":"datasetVersion","datasetId":4230180},{"sourceId":7379667,"sourceType":"datasetVersion","datasetId":4288572},{"sourceId":7516407,"sourceType":"datasetVersion","datasetId":4378189},{"sourceId":7516408,"sourceType":"datasetVersion","datasetId":4378190},{"sourceId":7516411,"sourceType":"datasetVersion","datasetId":4378191},{"sourceId":7516448,"sourceType":"datasetVersion","datasetId":4378213}],"dockerImageVersionId":30527,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport pandas as pd\nimport math\nfrom sklearn import svm, metrics\nfrom sklearn.datasets import load_digits\nfrom sklearn.cluster import KMeans\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.datasets import load_iris\nfrom sklearn.datasets._samples_generator import make_blobs\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix, r2_score, accuracy_score, f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnp.random.seed(0)\n\nimport seaborn as sns\n\nsns.set(font_scale=1.2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#variables to use in this kaggle\n(x,y,z) = 50,168.6,\"learning\"\nprint(x,y,z)\nA = \"indiaindiafkdjsgfdgdfindia\"\nprint(A)\nprint(A*7,'\\n')\nprint(A.replace('india',' REPLACED '))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('We want to show an example of Support Vector Machines for machine learning / data mining')\nimport pickle\n\nrecs = pd.read_csv(\"/kaggle/input/recipes-muffins-cupcakes-in-svm/recipes_muffins_cupcakes.csv\")\nprint(recs.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('We want to plot our data using SNS')\nsns.lmplot(data=recs, x='Flour', y='Sugar', hue='Type', palette='Set1', fit_reg=False, scatter_kws={\"s\": 70})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('the above plot shows some data as how it appears, though we want it to be more processed as to have it appear more precise')\ntype_label = np.where(recs['Type']=='Muffin',0,1)\nrec_features = recs.columns.values[1:].tolist()\nrec_features\ningredients = recs[['Flour','Sugar']].values\nprint(ingredients)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('now to get the model to fit to the pruned data')\nmodel = svm.SVC(kernel='linear')\nmodel.fit(ingredients,type_label)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('getting a hyperplane now')\nw = model.coef_[0]\na = -w[0] / w[1]\nxx = np.linspace(30,60)\nyy = a*xx - (model.intercept_[0] / w[1])\n\n\nb = model.support_vectors_[0]\nyy_down = a*xx+(b[1]-a*b[0])\nb = model.support_vectors_[-1]\nyy_up = a*xx+(b[1]-a*b[0])\n\nprint('this shows how the data can deliniate where the datapoints approach the plane')\nsns.lmplot(data=recs, x='Flour', y='Sugar', hue='Type', palette='Set1', fit_reg=False, scatter_kws={\"s\": 70})\nplt.plot(xx,yy,linewidth=2,color='black')\nplt.plot(xx,yy_down,'k--')\nplt.plot(xx,yy_up,'k--')\nplt.plot(50,20,'yo',markersize='9')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('lets create a predictive fucntion using flower and sugar')\ndef muff_or_cup(flour,sugar):\n    if(model.predict([[flour,sugar]]))==0:\n        print('Youre looking at some Muffins')\n    else:\n        print('Youre looking at some cupcakes!')\n\nprint('lets use some values 50 flour and 20 sugar, what will we get?',muff_or_cup(50,20) )\n#current time in video is around 1:19:55","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the dataset\ncars = pd.read_csv(\"/kaggle/input/cars-for-kmeans/cars (dataset for k means).csv\")\n\n# Handle missing values\n#this method will not work as it just blanket changes the items that are strings even though they are numbers, it errors out\n#x = cars.fillna(cars.mean())\n\n#better working version\nx = cars[cars.columns[:-1]]\n#x = x.convert_objects(convert_numeric=True) , this is deperciated now, use the following\nx= x._convert(numeric=True)\nx.head()\n\n#elm null values\nfor i in x.columns:\n    x[i] = x[i].fillna(int(x[i].mean()))\nfor i in x.columns:\n    print(x[i].isnull().sum())\n\n# Perform K-means clustering and find the optimal number of clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n\n# Plot the Elbow Method\nplt.plot(range(1, 11), wcss, marker='o')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\nkmeansA = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10, random_state=0)\ny_kmeans = kmeansA.fit_predict(x)\nx= x.values\nprint(y_kmeans,'\\n',x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.scatter(x[y_kmeans == 0,0], x[y_kmeans == 0,1],s=100,c='red',label='Toyota')\nplt.scatter(x[y_kmeans == 1,0], x[y_kmeans == 1,1],s=100,c='blue',label='Nissan')\nplt.scatter(x[y_kmeans == 2,0], x[y_kmeans == 2,1],s=100,c='green',label='Honda')\nplt.scatter(kmeans.cluster_centers_[:,0],kmeans.cluster_centers_[:,1],s=300,c='yellow',label='Centroids')\nplt.title('Clusters of car make')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we are going to user logestic regression to find some data on weather a tumor is malignent\ntumor = pd.read_csv('/kaggle/input/tumor-size/data (1) (dataset for logistic).csv')\nsns.jointplot(x='radius_mean',y='texture_mean',data = tumor)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(tumor.corr()) #plotting an unalterrted heatmap along with null values if any in the data\ntumor.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we will want to create an array with the worst possible data points and the diagnosis\nworstX=tumor[['radius_worst','texture_worst','perimeter_worst','area_worst','smoothness_worst','compactness_worst','concave points_worst','symmetry_worst','fractal_dimension_worst'\n]]\nworstY=tumor['diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(worstX, worstY, test_size=0.3, random_state=101)\n\n#running the model without a max iteration limit will cause it to error out hence the 1000\nlogModel = LogisticRegression(max_iter=1000)\nlogModel.fit(X_train, y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we do the predictions\ny_pred = logModel.predict(X_test)\nprint('the printer array shows how using a model it will classify if the tumor is benign or malignent')\nprint(y_pred)\nprint('\\nNow we will see how well the results match up\\n',classification_report(y_test,y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#some linear algebra refreshers\n\nmatA = np.array([10,15])\nmatB = np.array([20,9])\nmatD = np.dot(matA,matB) #this is the dot product in linear algebra\nprint(matA,'\\n',matB)\nmatC = matA.T+matB.T #using .T transposes the arrays\nprint(matC)\nprint((10*20)+(15*9))\nprint(matD)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets do some calculus, specifically finding the limit of something \n#neural networks achive the speed that they need by first going establishing layer, then differenting the diffrent layers\n# first layer will affect second layer and so on, this is proved by calculus\n#Gradiant Descent\n\ncur_x = 5 #algo starts at 3\nrate = 0.1\nprecision = 0.5 # how close it should be to stopping\nprevious_step_size = 1\nmax_iters = 1000000 # one million iterations\niters = 0\ndf = lambda x: 2*(x+5) # gradiant of our function\n\nwhile previous_step_size > precision and iters < max_iters:\n    prev_x = cur_x #storing current value\n    cur_x = cur_x - rate * df(prev_x) #evaluation the new value by the formula\n    previous_step_size = abs(cur_x - prev_x) # the new x\n    iters = iters +1 #iteration count\n    print('Iteration',iters,'\\nX value is',cur_x)\n    \nprint('The local minimum occurs at', cur_x)\n#scipi does this as a built in function buttt this can be more fun since you get to learn the diffrent functions\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfD = pd.DataFrame({'Name': ['Jane','Michael','Willian','Rosy','Hana','Ferdie','Graeme'],'Salary':[50000,54000,50000,189000,55000,40000,59000]})\nprint('Means',dfD['Salary'].mean())\nprint('Median', dfD['Salary'].median())\nprint('Mode',dfD['Salary'].mode())\n\n%matplotlib inline\nsalary = dfD['Salary']\nsalary.plot.hist(title='Salary Distribution', color='Grey',bins=25)\nplt.axvline(salary.mean(),color='violet',linestyle='dashed',linewidth=2)\nplt.axvline(salary.median(), color='red',linestyle='dashed',linewidth=2)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets create a set to show some probabilities\nset1 = {4,7}\nprint(set1)\n\n#now we convert that set into a list to change it around\nnewList = [1,2,3,4, 4]\nsetFromList = set(newList)\nprint(setFromList)\n\nfrom itertools import product as prod\n\nap = set([x for x in range(1,7)])\nbp = set([x for x in range(1,7)])\npp = list(prod(ap,bp))\n\nprint('Ap is the set of all possible outcomes of a dice: ',ap)\nprint('Bp is the set of all possible outcomes of a dice: ',bp)\nprint('products ap and bp are true for all possible combinations of Ap and Bp thrown together:\\n',pp)\n\n\n#we can toss two dice and see what happens here\n\nn_dice = 2\ndice_faces = {1,2,3,4,5,6}\n#now make the event space\n\nevent_space = set(prod(dice_faces, repeat=n_dice))\nevent_space\n\nprint('this is a new line')\nfor outcome in event_space:\n    print(outcome,end =', ')\nprint()\nprint(len(event_space))\n\n#now lets do it for a sum of something\nfaveOutcome = []\nfor outcome in event_space:\n    x,y = outcome\n    if (x+y)%3 == 0:\n        faveOutcome.append(outcome)\nfaveOutcome = set(faveOutcome)\n\nfor f_outcome in faveOutcome:\n    print(f_outcome, end=', ')\nprint()\nprint(len(faveOutcome))\n\nprob = len(faveOutcome)/len(event_space)\nprint('The probability of getting a sum which is a multiple of 3 is: ',prob)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now for a sum of a multiple of 5 but not of 3\nn_diceA = 5\ndice_faces = {1,2,3,4,5,6}\n#now make the event space\n\nevent_spaceA = set(prod(dice_faces, repeat=n_diceA))\nfaveOutcomes = []\nfor outcomesA in event_spaceA:\n    d1,d2,d3,d4,d5 = outcomesA\n    if (d1+d2+d3+d4+d5)%5 == 0 and (d1+d2+d3+d4+d5)%3!=0:\n        faveOutcomes.append(outcomesA)\nfaveOutcomes = set(faveOutcomes)\nlen(faveOutcomes)\nprobA = len(faveOutcomes)/len(event_spaceA)\nprint('The probability of getting a sum which is a multiple of 5 and not 3 is: ',probA)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#it is naive bayes time to learn and do\nsna = pd.read_csv('/kaggle/input/socialnetworkads/SocialNetworkAds.csv')\nprint(sna.head())\n\nxsna = sna.iloc[:,[2,3]].values\nysna = sna.iloc[:,4].values\n\nX_train, X_test, y_train, y_test = train_test_split(xsna,ysna,test_size = 0.25, random_state = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#time to feature scale\nscx = StandardScaler()\nX_train = scx.fit_transform(X_train)\nX_test = scx.transform(X_test)\n\nprint(X_train[:3])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we fit the Naive Bayes to the training set, lets see what outliers there are\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)\n\ny_pred = classifier.predict(X_test)\nprint('This should show how much is correct\\ntr is true pos, tl is false pos, br is false neg and bl is true neg')\ncm = confusion_matrix(y_test,y_pred)\ncm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we want to graph this as to show it better\nX_set, y_set = X_train, y_train\nX1, X2 = np.meshgrid(np.arange(start = X_set[:,0].min() - 1, stop = X_set[:,0].max() +1, step = 0.01),\n                    np.arange(start = X_set[:,1].min() - 1, stop = X_set[:,1].max() +1, step = 0.01))\nplt.contourf(X1,X2, classifier.predict(np.array([X1.ravel(),X2.ravel()]).T).reshape(X1.shape),\n             alpha = 0.75, cmap = ListedColormap(('red','green')))\nplt.xlim(X1.min(),X1.max())\nplt.ylim(X2.min(),X2.max())\n\nfor i, j in enumerate(np.unique(y_set)):\n    plt.scatter(X_set[y_set == j,0], X_set[y_set == j,1],\n               c=ListedColormap(('red','green'))(i), label = j)\n\nplt.title('Naive Bayes (Training Set)')\nplt.xlabel('Age')\nplt.ylabel('Estimated Salary')\nplt.legend()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#multiple linear regressions\n%matplotlib inline\ncompanies =pd.read_csv(\"/kaggle/input/1000companies/1000_Companies.csv\")\nxxx = companies.iloc[:, :-1].values #take everyrow except last one\nyyy = companies.iloc[:, :4].values #take every column except last 4\n\ncompanies.head()\nsns.heatmap(companies.corr())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\ncompanies =pd.read_csv(\"/kaggle/input/1000companies/1000_Companies.csv\")\nxxx = companies.iloc[:, :-1].values #take everyrow except last one\nyyy = companies.iloc[:, -1].values #take every column except last 4\n\nlabelencoder = LabelEncoder()\nxxx[:,3] = labelencoder.fit_transform(xxx[:,3])\n\n# Define the indices of categorical columns\ncategorical_features_indices = [3]\n\n# Create a ColumnTransformer for one-hot encoding\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('cat', OneHotEncoder(), categorical_features_indices)\n    ],\n    remainder='passthrough'  # You can specify other transformers for non-categorical columns\n)\n\n# Fit and transform your data\nxxx = preprocessor.fit_transform(xxx)\n#now we will remove the index column since its just false data\nxxx=xxx[:,1:]\n#its time to train it!\nX_train, X_test, y_train,y_test = train_test_split(xxx, yyy, test_size =0.2, random_state = 0)\nregressor = LinearRegression()\n#I have an error here where the table data cant be converted from a string to numerical, the onehotencoder should be rectifiying this\n#but currently it does not, need to investigate\n# Train the model\nregressor.fit(X_train, y_train)\n\n#previously we had included y_train but this gave off some simaler results however beacuse it was a 2d array\n#it could not be used for computing the R2 score which is only a 1d array algo\ny_pred = regressor.predict(X_test)\n\n# this fixes it yyy = companies.iloc[:, -1].values, taking out the : solved the issue\n#companies.head()\nprint(y_pred)\nprint(\"Time to print out the coefficients for y=mx+b:\",regressor.coef_)\nprint(\"This will calculate the intecept for y=mx+b:\",regressor.intercept_)\n\n# Obtain predictions using the trained model\n# Evaluate the model, this will show how close/accurte we are from predicited trained model to actual\nr2 = r2_score(y_test, y_pred)\nprint(\"R-squared score:\", r2)\n#a score of 91% is pretty decent confidence though these numbers could be a bit higher, to do that youll need to \n#include more data, more repetitions of the model to make it worth while\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#image recognition time! this uses a built in sklearn dataset for numbers\ndigits = load_digits()\n\nprint(\"Image data shape\", digits.data.shape)\nprint(\"Label data shape\", digits.target.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#what this will do is show the plots of training data of numbers that will show the shapes\nplt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(digits.data[0:5], digits.target[0:5])):\n    plt.subplot(1,5,index+1)\n    plt.imshow(np.reshape(image,(8,8)),cmap=plt.cm.gray)\n    plt.title('training: %i\\n' % label, fontsize = 20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#the data will now be trained to show how good it looks\nx_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.23, random_state=2)\nprint(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#implementiing the data that was trained from the digits dataset\n#this comes back with a max iterations so will have to be redone to boost the iteration sup to 1000 maybe?\n\nlogisticRegr = LogisticRegression(max_iter=2000)\nlogisticRegr.fit(x_train,y_train)\nprint(logisticRegr.predict(x_test[1].reshape(1,-1)))\n\nlogisticRegr.predict(x_test[0:10])\npredictNum = logisticRegr.predict(x_test)\n\nscores = logisticRegr.score(x_test,y_test)\nprint(\"Accuracy:\",scores)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this is to show the confusion matrix based on how accurate the model trained\ncm = metrics.confusion_matrix(y_test,predictNum)\nprint(\"Confusion Matrix for number prediction\\n\",cm)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('New plot')\nplt.figure(figsize=(9,9))\nsns.heatmap(cm,annot=True,fmt=\".3f\",linewidth=.5,square=True,cmap='Blues_r');\nplt.ylabel('Actual Label')\nplt.xlabel('Predicted Label')\nall_sample_title = 'Accuracy Score: {0}'.format(scores)\nplt.title(all_sample_title, size= 15)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('now another graph, this will show predicted vs actual')\nindex = 0\nclassifiedIndex = []\nfor predict, actual in zip(predictNum, y_test):\n    if predict == actual:\n        classifiedIndex.append(index)\n    index += 1\n\nplt.figure(figsize=(20,3))\nfor plotIndex, wrong in enumerate(classifiedIndex[0:4]):\n    plt.subplot(1,4,plotIndex +1)\n    plt.imshow(np.reshape(x_test[wrong],(8,8)), cmap=plt.cm.gray)\n    plt.title(\"predicted: {}, Actual: {}\".format(predictNum[wrong],y_test[wrong]),fontsize=20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this will be a mainly confusion matrix focused demo\nheart =pd.read_csv(\"/kaggle/input/heart-disease-cleveland-uci/heart_cleveland_upload.csv\")\nprint('column 13 has condition instead of target, also the data seems to be a little big off')\nheart\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we are dropping the last column as the data is not needed\n#heart = data, yH & xH are to diffrenetiate from the above variables\nyH = heart['condition']\nxH = heart.drop('condition',axis = 1)\nprint(xH.head())\n\n#this will use a standard training model for preprocessing, nothing fancy, 20% test size 80% train\nX_train, X_test, y_train, y_test, = train_test_split(xH,yH, test_size = 0.2, random_state = 42)\n\nscalerH = StandardScaler()\nscaleH = scalerH.fit(X_train)\nX_train = scaleH.transform(X_train)\nX_test = scaleH.transform(X_test)\n\n#now creating a standard logistic regression\nmodelH = LogisticRegression()\nmodelH.fit(X_train,y_train)\npredH = modelH.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this is to get the accuracy score only for our model\nscoreH = accuracy_score(y_test,predH)\nprint('This is how accurate our model is based on our trained data from the UCI heart CSV: ',scoreH)\n\n#now well need the confusion matrix to get all the good stuff\nprint('Now viewing this data as such can be a little confusing, thankfully we can pull it apart just below')\nconfusion_matrix(y_test,predH)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tn,fp,fn,tp = confusion_matrix(y_test,predH).ravel()\nprint('Here is our True Negative: ',tn,'\\nHere is our False Negative: ',fn,'\\nHere is our True Positive: ',tp,'\\nHere is our False Positive: ',fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Welcome one and all to a python decsion tree that will see if a car loan will default!\n\nbalance_data = pd.read_csv('/kaggle/input/car-loan-decision-tree-repayment/CarLoanRepaymentDecisionTree.csv',sep = ',', header = 0)\nbalance_data = balance_data.drop('Unnamed: 5', axis=1)\nbalance_data.head() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we want to find out the length of the CSV file since it contains over 1000 records\nprint('Dataset Length:: ',len(balance_data))\nprint('Dataset Shape:: ',balance_data.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#to figure out how to train the Decision tree, the data itself has to be sifted to figure out what will give out the max gain\nxCarLoan = balance_data.values[:,1:5]\nyCarLoan = balance_data.values[:,0]\n\n#splitting our dataset up as one does to train and test it against it self\nx_train, x_test, y_train, y_test = train_test_split(xCarLoan,yCarLoan, test_size=0.3, random_state=100)\n\n#this function will find the entropy of a system based on how the data can be shuffled \nclf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5)\nclf_entropy.fit(x_train, y_train)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#We will want to predict something with this data now\ny_pred_en = clf_entropy.predict(x_test)\ny_pred_en","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now we will see the accuracy, it is somewhat on point but could be better\nprint('Accuracy of this Car Loan model is : ',accuracy_score(y_test,y_pred_en)*100)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Delving into a Random Forest now\n\nirisA = load_iris()\nirisB = pd.read_csv('/kaggle/input/iris-random-forest/iris.csv')\n\n#for iA we are following the built in data set\n#for iB we are using a downloaded version, this will help compare the data between the two\niA = pd.DataFrame(irisA.data,columns=irisA.feature_names)\niB = pd.DataFrame(irisB)\niB = iB.drop('Id', axis=1)\n\n\niA.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"iB.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#it appears we are missing a column in the built in database iris whereas the downloaded copy of iris already shows it\n#lets add the species so we know whats happeneing\n\niA['Species'] = pd.Categorical.from_codes(irisA.target,irisA.target_names)\niA.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#here we do a train/test with the built in database\niA['is_train'] = np.random.uniform(0,1,len(iA)) <= .75\niA.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Now this is with the downloaded database, take a look at the is_train column\niB['is_train'] = np.random.uniform(0,1,len(iB)) <= .75\niB.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets create new dataframes for our trained data\ntrainA, testA = iA[iA['is_train']==True], iA[iA['is_train']==False]\ntrainB, testB = iB[iB['is_train']==True], iB[iB['is_train']==False]\n\nprint('The number of observations in the built in training data:',len(trainA),'\\nAs well as the number of observations in the test data:',len(testA))\nprint('\\n')\nprint('This is the trained data from a downloaded copy of Iris, the training data:', len(trainB),'\\nHere is the tested data:',len(testB))\n\n\ndef trainPercent(trainA, trainB):\n    max_value = max(len(trainA), len(trainB))\n    similarity = 1 - abs(len(trainA) - len(trainB)) / max_value\n    percentA = similarity * 100\n\n    return percentA\n\n# Similarly, modify the testPercent function\ndef testPercent(testA, testB):\n    max_value = max(len(trainA), len(trainB))\n    similarity = 1 - abs(len(trainA) - len(trainB)) / max_value\n    percentB = similarity * 100\n\n    return percentB\n\n\nprint('\\nNow beacuse we want to calculate to see the similarity of the two databases\\nWe ran the numbers thru a small function to calculate that')\nprint('The train data percentage: ',trainPercent(trainA,trainB),'\\nThe test data percentage:  ',testPercent(testA, testB))\nprint('In the end, both datasets are pretty simaler though right now they arent really quantified wihtout units')\n\nfeaturesA = iA.columns[:4]\nfeaturesB = iB.columns[:4]\n\n#converting the genus/species into digits for easier quantifying. Allows the computer to understand it\nyA = pd.factorize(trainA['Species'])[0]\nyB = pd.factorize(trainB['Species'])[0]\n\nprint(yA)\nprint(yB)\n\n#random forest classifer\n#after cleaning up the data and getting it ready, now we can implement the classifer for a good prediction\n\nclfA = RandomForestClassifier(n_jobs=2, random_state=0)\nclfB = RandomForestClassifier(n_jobs=2, random_state=0)\n\nclfA.fit(trainA[featuresA],yA)\nclfB.fit(trainB[featuresB],yB)\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#implementing the random forest\nprint('Built in database',clfA.predict(testA[featuresA]),'\\n')\nprint('Downloaded File',clfB.predict(testB[featuresB]),'\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Built in database\\n',clfA.predict_proba(testA[featuresA])[0:10],'\\n')\nprint('Downloaded database\\n',clfB.predict_proba(testB[featuresB])[0:10],'\\n')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#lets map some plant names so that we know what could come up, this will be for the first 5 observations\npredsA = irisA.target_names[clfA.predict(testA[featuresA])]\n#predsB = irisB.target_names[clfB.predict(testB[featuresB])], there is target_names in the downloaded dataset\npredsB = irisA.target_names[clfB.predict(testB[featuresB])]\n\nprint(predsA[0:25],'\\n')\nprint(predsB[0:25],'\\n')\n\nprint('Viewing Actual data, not test\\n',testA['Species'].head(),'\\n\\nActual data from the download\\n',testB['Species'].head())\n\n#now lets combine actual and test data into one confusion matrix!\nirisCMA = pd.crosstab(testA['Species'],predsA, rownames=['Actual Species'], colnames=['Predicted Species'])\nirisCMB = pd.crosstab(testB['Species'],predsB, rownames=['Actual Species'], colnames=['Predicted Species'])\nprint('\\nThis is the built in database for Iris\\n',irisCMA,'\\n\\nThis is the downloaded database\\n',irisCMB)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#KNN model of diabetes\n\ndiabetes = pd.read_csv('/kaggle/input/knndiabetes/diabetes.csv')\n#time to replace the zeros with averaged data, this can skew the outcome of the KNN\n\nzero_not_accepted = ['Glucose','BloodPressure','SkinThickness','BMI','Insulin']\nfor column in zero_not_accepted:\n    diabetes[column] = diabetes[column].replace(0, np.NaN)\n    mean = int(diabetes[column].mean(skipna=True))\n    diabetes[column] = diabetes[column].replace(np.NaN, mean)\n\n#now lets split up the adjusted dataset to train our model\n\nxD = diabetes.iloc[:, 0:8]\nyD = diabetes.iloc[:, 8]\nX_train, X_test, y_train, y_test = train_test_split(xD, yD, random_state=0, test_size=0.2)\n\n#this is to expand the data so that it is uniform\nsc_xD = StandardScaler()\nX_train = sc_xD.fit_transform(X_train)\nX_test = sc_xD.transform(X_test)\n\nclassifierD = KNeighborsClassifier(n_neighbors = 11, p=2, metric='euclidean')\nclassifierD.fit(X_train,y_train)\n\npredictD = classifierD.predict(X_test)\n\ncmD = confusion_matrix(y_test, predictD)\nprint('Firstly, the confusion matrix, down diagnoal towards the right indicates correctness\\n',cmD)\n\nprint('\\nHere is our F1 score: \\n',f1_score(y_test,predictD))\nprint('\\nHere is the accuracy of the model: \\n',accuracy_score(y_test,predictD))\nprint('\\nOverall the model with the limited data was somewhat realistic but it could use more to chrun through')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Support Vector Machines \n#another staple of Machine Learning programs, SVM's can help to transform data thats usually massive, needs to have its bias cut or simply want a stable algo\n\n#this is a sample run of SVM using the built in blob database in order to sort them out and see whos the blobbiest\n\n#plain ol variables, we want 800 intakes and 15 types of blobs\nxBlob, yBlob = make_blobs(n_samples = 80, centers = 7, random_state = 20)\n#having the SVM algo create from a 1D mixxed bag to a 2D graph\nclfBlob = svm.SVC(kernel = 'linear', C=1000)\nclfBlob.fit(xBlob,yBlob)\n\nplt.scatter(xBlob[:,0], xBlob[:,1], c=yBlob, s=30, cmap=plt.cm.Paired)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now that weve established and ran the SVM on the blob data, we will need to do some new additional data to be predicted\nnewBlobData = [[3,4],[5,6]]\nprint(clfBlob.predict(newBlobData))\n#more or less shows that its accurate to a degree but its just blobs so lets make something more useful","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.datasets._samples_generator import make_blobs\nclfNew = svm.SVC(kernel = 'linear', C = 1000)\nclfNew.fit(xBlob,yBlob)\nplt.scatter(xBlob[:,0],xBlob[:,1], c=yBlob, s=30, cmap=plt.cm.Paired)\nplt.show\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# the memoryy hog code\n#ill have to run this locally as kaggle runs out of memory for the free version at least, 30gbs+ of ram!\nax = plt.gca()\nxlimB = ax.get_xlim()\nylimB = ax.get_ylim()\n\nxxB = np.linspace(xlimB[0],xlimB[1],30000)\nyyB = np.linspace(ylimB[0], ylimB[1],30000)\nxxB, yyB = np.meshgrid(yyB,xxB)\nxyB = np.vstack([xxB.ravel(),yyB.ravel()]).T\nzB = clfNew.decision_function(xyB).reshape(xxB.shape)\n\nax.contour (xxB, yyB, zB, colors = 'k', levels=[-1,0,1],\n           alpha = 0.5,\n           linestyles=['--','-','--'])\n\nax.scatter(clfNew.support_vectors_[:,0],\n          clfNew.support_vectors_[:,1], s=100,\n          linewidth=1, facecolors='none')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now applying a multiple linear regression model\n\"\"\"Taking the data from California Housing, we train 75% and test the rest to get a good idea on what the dataset is\nThere is an error check done so as to verify that the data aint to far off skewed, it comes back at 0.506 which is a little over half wrong?\nI suppose thats fine depending on your standards\n\nThe rest of the code then just spurts out some coefficent percentages\"\"\"\n\nlreg = LinearRegression()\nlreg.fit(x_train,y_train)\n\nlreg_y_pred = lreg.predict(x_test)\n\nmean_squared_error = np.mean((lreg_y_pred - y_test)**2)\nprint('Mean squared error on test set: ', mean_squared_error)\n\nlreg_coefficient = pd.DataFrame()\nlreg_coefficient[\"Columns\"] = x_train.columns\nlreg_coefficient['Coefficient estimate'] = pd.Series(lreg.coef_)\nprint(lreg_coefficient)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import fetch_california_housing\nboston_dataset = fetch_california_housing()\n\n#had to use a differnt data set since boston is no longer in the sklearn framework\n#this was mostly due to some racial data, which I suppose is fine but what if a person wants to use it since it has that?\n\nboston_pd = pd.DataFrame(boston_dataset.data)\nboston_pd.columns = boston_dataset.feature_names\nboston_pd_target = np.asarray(boston_dataset.target)\nboston_pd['House Price'] = pd.Series(boston_pd_target)\n\nXBos = boston_pd.iloc[:, :-1]\nYBos = boston_pd.iloc[:, -1]\n\nprint(boston_pd.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(boston_pd.iloc[:, :-1], boston_pd.iloc[:, -1], test_size = 0.25)\nprint('Train data shape of x = % s and Y = % s : '%(x_train.shape, y_train.shape))\nprint('Test data shape of X = % s and Y = % s : '%(x_test.shape, y_test.shape))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#now to apply a multiple level linear regression model\n#it will error check the test data, which makes it lean a little skewed & bias\n#then it will print out all the coefficient data that we need to adjust\n\nlreg = LinearRegression()\nlreg.fit(x_train,y_train)\n\nlreg_y_pred = lreg.predict(x_test)\n\nmean_squared_error = np.mean((lreg_y_pred - y_test)**2)\nprint('Mean squared error on the test set: ',mean_squared_error)\n\nlreg_coefficient = pd.DataFrame()\nlreg_coefficient['Columns'] = x_train.columns\nlreg_coefficient['Coefficent Estimate'] = pd.Series(lreg.coef_)\nprint(lreg_coefficient)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,10))\ncolor = ['tab:gray', 'tab:blue', 'tab:orange','tab:green','tab:red', 'tab:brown','tab:pink', 'tab:gray','tab:olive','tab:cyan','tab:orange','tab:green','tab:blue','tab:olive']\n\nax.bar(lreg_coefficient['Columns'], lreg_coefficient['Coefficent Estimate'], color = color)\nax.spines['bottom'].set_position('zero')\n\nplt.style.use('ggplot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Ridge\nridgeR = Ridge(alpha = 1)\nridgeR.fit(x_train,y_train)\ny_pred = ridgeR.predict(x_test)\n\nmean_squared_error_ridge = np.mean((y_pred - y_test)**2)\nprint('Mean squared error on test set with Ridge', mean_squared_error_ridge)\n\nridge_coefficient = pd.DataFrame()\nridge_coefficient['Columns'] = x_train.columns\nridge_coefficient['Coefficent Estimate'] = pd.Series(ridgeR.coef_)\n\nprint(ridge_coefficient)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,10))\ncolor = ['tab:gray', 'tab:blue', 'tab:orange','tab:green','tab:red', 'tab:brown','tab:pink', 'tab:gray','tab:olive','tab:cyan','tab:orange','tab:green','tab:blue','tab:olive']\n\nax.bar(ridge_coefficient['Columns'], ridge_coefficient['Coefficent Estimate'], color = color)\nax.spines['bottom'].set_position('zero')\n\nplt.style.use('ggplot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 1)\nlasso.fit(x_train,y_train)\ny_pred1 = lasso.predict(x_test)\n\nmean_squared_error_lasso = np.mean((y_pred1 - y_test)**2)\nprint('Mean squared error on test set with Lasso', mean_squared_error_lasso)\n\nlasso_coefficient = pd.DataFrame()\nlasso_coefficient['Columns'] = x_train.columns\nlasso_coefficient['Coefficent Estimate'] = pd.Series(lasso.coef_)\n\nprint(lasso_coefficient)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (20,10))\ncolor = ['tab:gray', 'tab:blue', 'tab:orange','tab:green','tab:red', 'tab:brown','tab:pink', 'tab:gray','tab:olive','tab:cyan','tab:orange','tab:green','tab:blue','tab:olive']\n\nax.bar(ridge_coefficient['Columns'], lasso_coefficient['Coefficent Estimate'], color = color)\nax.spines['bottom'].set_position('zero')\n\nplt.style.use('ggplot')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#An example of Principal Component Analysis\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\ncancer.keys()\n\ndfc = pd.DataFrame(cancer['data'],columns = cancer['feature_names'])\ndfc.head()\n\nfrom sklearn.preprocessing import StandardScaler\nscalerC = StandardScaler()\nscalerC.fit(dfc)\nscaled_data = scalerC.transform(dfc)\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 2)\npca.fit(scaled_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this took 30 features and smashed them down to two features\nx_pca = pca.transform(scaled_data)\nprint(scaled_data.shape)\nprint(x_pca.shape)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,6))\nplt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='plasma')\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component')\n#this graph shows two components from the previous 30, easy to show but hard to display all that info","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#we can show the individual components themselves and how often they occur though\ndf_comp = pd.DataFrame(pca.components_,columns=cancer[\"features_name\"])\nplt.figure(figsize=(12,6))\nsns.heatmap(df_comp,cmap='plasma')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#final implmentation of covid prediction analysis\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport random\nimport math\nimport time\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nimport datetime\nimport operator\nplt.style.use('fivethirtyeight')\n%matplotlib inline\n\n#data was initally sorted in the JHU CSSE repossirtoy but beacuase of the way it was stored, it may have been too long a path\nconfirmed_cases = pd.read_csv('/kaggle/input/jhu-csse-covid19-confirmed/time_series_covid19_confirmed_global.csv')\ndeaths_reported = pd.read_csv('/kaggle/input/jhu-csse-covid19-deaths/time_series_covid19_deaths_global.csv')\nrecovered_cases = pd.read_csv('/kaggle/input/jhu-csse-covid19-recovered/time_series_covid19_recovered_global.csv')\nlatest_data = pd.read_csv('/kaggle/input/jhu-csse-covid19-june19th2022report/06-19-2022.csv')\n\n#editing columns to prepare the data, this will use the confiremd cases column as the main index for the other two\ncols = confirmed_cases.keys()\n\n#only allowing certain columns to come thru\nconfirmed = confirmed_cases.loc[:, cols[4]:cols[-1]]\ndeaths = deaths_reported.loc[:, cols[4]:cols[-1]]\nrecoveries = recovered_cases.loc[:, cols[4]:cols[-1]]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:51:12.192625Z","iopub.execute_input":"2024-02-07T22:51:12.193006Z","iopub.status.idle":"2024-02-07T22:51:12.977885Z","shell.execute_reply.started":"2024-02-07T22:51:12.192979Z","shell.execute_reply":"2024-02-07T22:51:12.976822Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"#huge dump of empty lists to showcase all the countries\ndates = confirmed.keys()\nworld_cases = []\ntotal_deaths = []\nmortality_rate = []\nrecovery_rate = []\ntotal_recovered = []\ntotal_active = []\n\nchina_cases = []\nitaly_cases = []\nus_cases = []\nspain_cases = []\nfrance_cases = []\ngermany_cases = []\nuk_cases = []\nrussia_cases = []\nindia_cases = []\n\nchina_deaths = []\nitaly_deaths = []\nus_deaths = []\nspain_deaths = []\nfrance_deaths = []\ngermany_deaths = []\nuk_deaths = []\nrussia_deaths = []\nindia_deaths = []\n\nchina_recoveries = []\nitaly_recoveries = []\nus_recoveries = []\nspain_recoveries = []\nfrance_recoveries = []\ngermany_recoveries = []\nuk_recoveries = []\nrussia_recoveries = []\nindia_recoveries = []","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:51:15.000506Z","iopub.execute_input":"2024-02-07T22:51:15.000853Z","iopub.status.idle":"2024-02-07T22:51:15.007943Z","shell.execute_reply.started":"2024-02-07T22:51:15.000825Z","shell.execute_reply":"2024-02-07T22:51:15.006957Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#giant for loop to dump data inside\n\nfor i in dates:\n    confirmed_sum = confirmed[i].sum()\n    death_sum = deaths[i].sum()\n    recovered_sum = recoveries[i].sum()\n    \n    world_cases.append(confirmed_sum)\n    total_deaths.append(death_sum)\n    total_recovered.append(recovered_sum)\n    total_active.append(confirmed_sum-death_sum-recovered_sum)\n    \n    mortality_rate.append(death_sum/confirmed_sum)\n    recovery_rate.append(recovered_sum/confirmed_sum)\n    \n    china_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='China'][i].sum())\n    italy_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='Italy'][i].sum())\n    us_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='US'][i].sum())\n    spain_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='Spain'][i].sum())\n    france_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='France'][i].sum())\n    germany_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='Germany'][i].sum())\n    uk_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='United Kingdom'][i].sum())\n    russia_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='Russia'][i].sum())\n    india_cases.append(confirmed_cases[confirmed_cases['Country/Region']=='India'][i].sum())\n    \n    china_deaths.append(deaths_reported[deaths_reported['Country/Region']=='China'][i].sum())\n    italy_deaths.append(deaths_reported[deaths_reported['Country/Region']=='Italy'][i].sum())\n    us_deaths.append(deaths_reported[deaths_reported['Country/Region']=='US'][i].sum())\n    spain_deaths.append(deaths_reported[deaths_reported['Country/Region']=='Spain'][i].sum())\n    france_deaths.append(deaths_reported[deaths_reported['Country/Region']=='France'][i].sum())\n    germany_deaths.append(deaths_reported[deaths_reported['Country/Region']=='Germany'][i].sum())\n    uk_deaths.append(deaths_reported[deaths_reported['Country/Region']=='United Kingdom'][i].sum())\n    russia_deaths.append(deaths_reported[deaths_reported['Country/Region']=='Russia'][i].sum())\n    india_deaths.append(deaths_reported[deaths_reported['Country/Region']=='India'][i].sum())\n    \n    china_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='China'][i].sum())\n    italy_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='Italy'][i].sum())\n    us_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='US'][i].sum())\n    spain_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='Spain'][i].sum())\n    france_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='France'][i].sum())\n    germany_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='Germany'][i].sum())\n    uk_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='United Kingdom'][i].sum())\n    russia_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='Russia'][i].sum())\n    india_recoveries.append(recovered_cases[recovered_cases['Country/Region']=='India'][i].sum())","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:51:43.103338Z","iopub.execute_input":"2024-02-07T22:51:43.103754Z","iopub.status.idle":"2024-02-07T22:51:56.385825Z","shell.execute_reply.started":"2024-02-07T22:51:43.103720Z","shell.execute_reply":"2024-02-07T22:51:56.384900Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def daily_increase(data):\n    d = []\n    for i in range(len(data)):\n        if i == 0:\n            d.append(data[0])\n        else:\n            d.append(data[i]-data[i-1])\n    return d","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:52:00.110956Z","iopub.execute_input":"2024-02-07T22:52:00.111581Z","iopub.status.idle":"2024-02-07T22:52:00.116935Z","shell.execute_reply.started":"2024-02-07T22:52:00.111542Z","shell.execute_reply":"2024-02-07T22:52:00.116244Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#this snippet is to confirm the already coinfirmed cases\n\nworld_daily_increase = daily_increase(world_cases)\nchina_daily_increase = daily_increase(china_cases)\nitaly_daily_increase = daily_increase(italy_cases)\nus_daily_increase = daily_increase(us_cases)\nspain_daily_increase = daily_increase(spain_cases)\nfrance_daily_increase = daily_increase(france_cases)\ngermany_daily_increase = daily_increase(germany_cases)\nuk_daily_increase = daily_increase(uk_cases)\nrussia_daily_increase = daily_increase(russia_cases)\nindia_daily_increase = daily_increase(india_cases)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:52:05.371133Z","iopub.execute_input":"2024-02-07T22:52:05.371485Z","iopub.status.idle":"2024-02-07T22:52:05.381189Z","shell.execute_reply.started":"2024-02-07T22:52:05.371458Z","shell.execute_reply":"2024-02-07T22:52:05.380113Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#checking to see the daily deaths in july\nworld_daily_death = daily_increase(total_deaths)\nchina_daily_death = daily_increase(china_deaths)\nitaly_daily_death = daily_increase(italy_deaths)\nus_daily_death = daily_increase(us_deaths)\nspain_daily_death = daily_increase(spain_deaths)\nfrance_daily_death = daily_increase(france_deaths)\ngermany_daily_death = daily_increase(germany_deaths)\nuk_daily_death = daily_increase(uk_deaths)\nrussia_daily_death = daily_increase(russia_deaths)\nindia_daily_death = daily_increase(india_deaths)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:52:08.099839Z","iopub.execute_input":"2024-02-07T22:52:08.100168Z","iopub.status.idle":"2024-02-07T22:52:08.108471Z","shell.execute_reply.started":"2024-02-07T22:52:08.100144Z","shell.execute_reply":"2024-02-07T22:52:08.107236Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#finally seeing how many have recovered\nworld_daily_recovery = daily_increase(total_recovered)\nchina_daily_recovery = daily_increase(china_recoveries)\nitaly_daily_recovery = daily_increase(italy_recoveries)\nus_daily_recovery = daily_increase(us_recoveries)\nspain_daily_recovery = daily_increase(spain_recoveries)\nfrance_daily_recovery = daily_increase(france_recoveries)\ngermany_daily_recovery = daily_increase(germany_recoveries)\nuk_daily_recovery = daily_increase(uk_recoveries)\nrussia_daily_recovery = daily_increase(russia_recoveries)\nindia_daily_recovery = daily_increase(india_recoveries)","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:52:13.781139Z","iopub.execute_input":"2024-02-07T22:52:13.781497Z","iopub.status.idle":"2024-02-07T22:52:13.790921Z","shell.execute_reply.started":"2024-02-07T22:52:13.781468Z","shell.execute_reply":"2024-02-07T22:52:13.789777Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#we want to check unique countries for some of the latest data\nunique_countries = list(latest_data['Country_Region'].unique())\nunique_countries","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:52:15.880016Z","iopub.execute_input":"2024-02-07T22:52:15.880375Z","iopub.status.idle":"2024-02-07T22:52:15.892798Z","shell.execute_reply.started":"2024-02-07T22:52:15.880349Z","shell.execute_reply":"2024-02-07T22:52:15.891562Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"['Afghanistan',\n 'Albania',\n 'Algeria',\n 'Andorra',\n 'Angola',\n 'Antarctica',\n 'Antigua and Barbuda',\n 'Argentina',\n 'Armenia',\n 'Australia',\n 'Austria',\n 'Azerbaijan',\n 'Bahamas',\n 'Bahrain',\n 'Bangladesh',\n 'Barbados',\n 'Belarus',\n 'Belgium',\n 'Belize',\n 'Benin',\n 'Bhutan',\n 'Bolivia',\n 'Bosnia and Herzegovina',\n 'Botswana',\n 'Brazil',\n 'Brunei',\n 'Bulgaria',\n 'Burkina Faso',\n 'Burma',\n 'Burundi',\n 'Cabo Verde',\n 'Cambodia',\n 'Cameroon',\n 'Canada',\n 'Central African Republic',\n 'Chad',\n 'Chile',\n 'China',\n 'Colombia',\n 'Comoros',\n 'Congo (Brazzaville)',\n 'Congo (Kinshasa)',\n 'Costa Rica',\n \"Cote d'Ivoire\",\n 'Croatia',\n 'Cuba',\n 'Cyprus',\n 'Czechia',\n 'Denmark',\n 'Diamond Princess',\n 'Djibouti',\n 'Dominica',\n 'Dominican Republic',\n 'Ecuador',\n 'Egypt',\n 'El Salvador',\n 'Equatorial Guinea',\n 'Eritrea',\n 'Estonia',\n 'Eswatini',\n 'Ethiopia',\n 'Fiji',\n 'Finland',\n 'France',\n 'Gabon',\n 'Gambia',\n 'Georgia',\n 'Germany',\n 'Ghana',\n 'Greece',\n 'Grenada',\n 'Guatemala',\n 'Guinea',\n 'Guinea-Bissau',\n 'Guyana',\n 'Haiti',\n 'Holy See',\n 'Honduras',\n 'Hungary',\n 'Iceland',\n 'India',\n 'Indonesia',\n 'Iran',\n 'Iraq',\n 'Ireland',\n 'Israel',\n 'Italy',\n 'Jamaica',\n 'Japan',\n 'Jordan',\n 'Kazakhstan',\n 'Kenya',\n 'Kiribati',\n 'Korea, North',\n 'Korea, South',\n 'Kosovo',\n 'Kuwait',\n 'Kyrgyzstan',\n 'Laos',\n 'Latvia',\n 'Lebanon',\n 'Lesotho',\n 'Liberia',\n 'Libya',\n 'Liechtenstein',\n 'Lithuania',\n 'Luxembourg',\n 'MS Zaandam',\n 'Madagascar',\n 'Malawi',\n 'Malaysia',\n 'Maldives',\n 'Mali',\n 'Malta',\n 'Marshall Islands',\n 'Mauritania',\n 'Mauritius',\n 'Mexico',\n 'Micronesia',\n 'Moldova',\n 'Monaco',\n 'Mongolia',\n 'Montenegro',\n 'Morocco',\n 'Mozambique',\n 'Namibia',\n 'Nepal',\n 'Netherlands',\n 'New Zealand',\n 'Nicaragua',\n 'Niger',\n 'Nigeria',\n 'North Macedonia',\n 'Norway',\n 'Oman',\n 'Pakistan',\n 'Palau',\n 'Panama',\n 'Papua New Guinea',\n 'Paraguay',\n 'Peru',\n 'Philippines',\n 'Poland',\n 'Portugal',\n 'Qatar',\n 'Romania',\n 'Russia',\n 'Rwanda',\n 'Saint Kitts and Nevis',\n 'Saint Lucia',\n 'Saint Vincent and the Grenadines',\n 'Samoa',\n 'San Marino',\n 'Sao Tome and Principe',\n 'Saudi Arabia',\n 'Senegal',\n 'Serbia',\n 'Seychelles',\n 'Sierra Leone',\n 'Singapore',\n 'Slovakia',\n 'Slovenia',\n 'Solomon Islands',\n 'Somalia',\n 'South Africa',\n 'South Sudan',\n 'Spain',\n 'Sri Lanka',\n 'Sudan',\n 'Summer Olympics 2020',\n 'Suriname',\n 'Sweden',\n 'Switzerland',\n 'Syria',\n 'Taiwan*',\n 'Tajikistan',\n 'Tanzania',\n 'Thailand',\n 'Timor-Leste',\n 'Togo',\n 'Tonga',\n 'Trinidad and Tobago',\n 'Tunisia',\n 'Turkey',\n 'US',\n 'Uganda',\n 'Ukraine',\n 'United Arab Emirates',\n 'United Kingdom',\n 'Uruguay',\n 'Uzbekistan',\n 'Vanuatu',\n 'Venezuela',\n 'Vietnam',\n 'West Bank and Gaza',\n 'Winter Olympics 2022',\n 'Yemen',\n 'Zambia',\n 'Zimbabwe',\n 'Nauru',\n 'Tuvalu']"},"metadata":{}}]},{"cell_type":"code","source":"latest_data.head()\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:52:20.407172Z","iopub.execute_input":"2024-02-07T22:52:20.408538Z","iopub.status.idle":"2024-02-07T22:52:20.427283Z","shell.execute_reply.started":"2024-02-07T22:52:20.408496Z","shell.execute_reply":"2024-02-07T22:52:20.426285Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"   FIPS Admin2 Province_State Country_Region          Last_Update       Lat  \\\n0   NaN    NaN            NaN    Afghanistan  2022-06-20 04:21:01  33.93911   \n1   NaN    NaN            NaN        Albania  2022-06-20 04:21:01  41.15330   \n2   NaN    NaN            NaN        Algeria  2022-06-20 04:21:01  28.03390   \n3   NaN    NaN            NaN        Andorra  2022-06-20 04:21:01  42.50630   \n4   NaN    NaN            NaN         Angola  2022-06-20 04:21:01 -11.20270   \n\n       Long_  Confirmed  Deaths  Recovered  Active Combined_Key  \\\n0  67.709953     181666    7713        NaN     NaN  Afghanistan   \n1  20.168300     277409    3497        NaN     NaN      Albania   \n2   1.659600     265975    6875        NaN     NaN      Algeria   \n3   1.521800      43449     153        NaN     NaN      Andorra   \n4  17.873900      99761    1900        NaN     NaN       Angola   \n\n   Incident_Rate  Case_Fatality_Ratio  \n0     466.667716             4.245704  \n1    9639.620543             1.260594  \n2     606.542015             2.584829  \n3   56233.741021             0.352137  \n4     303.536136             1.904552  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>FIPS</th>\n      <th>Admin2</th>\n      <th>Province_State</th>\n      <th>Country_Region</th>\n      <th>Last_Update</th>\n      <th>Lat</th>\n      <th>Long_</th>\n      <th>Confirmed</th>\n      <th>Deaths</th>\n      <th>Recovered</th>\n      <th>Active</th>\n      <th>Combined_Key</th>\n      <th>Incident_Rate</th>\n      <th>Case_Fatality_Ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Afghanistan</td>\n      <td>2022-06-20 04:21:01</td>\n      <td>33.93911</td>\n      <td>67.709953</td>\n      <td>181666</td>\n      <td>7713</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Afghanistan</td>\n      <td>466.667716</td>\n      <td>4.245704</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Albania</td>\n      <td>2022-06-20 04:21:01</td>\n      <td>41.15330</td>\n      <td>20.168300</td>\n      <td>277409</td>\n      <td>3497</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Albania</td>\n      <td>9639.620543</td>\n      <td>1.260594</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Algeria</td>\n      <td>2022-06-20 04:21:01</td>\n      <td>28.03390</td>\n      <td>1.659600</td>\n      <td>265975</td>\n      <td>6875</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Algeria</td>\n      <td>606.542015</td>\n      <td>2.584829</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Andorra</td>\n      <td>2022-06-20 04:21:01</td>\n      <td>42.50630</td>\n      <td>1.521800</td>\n      <td>43449</td>\n      <td>153</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Andorra</td>\n      <td>56233.741021</td>\n      <td>0.352137</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Angola</td>\n      <td>2022-06-20 04:21:01</td>\n      <td>-11.20270</td>\n      <td>17.873900</td>\n      <td>99761</td>\n      <td>1900</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Angola</td>\n      <td>303.536136</td>\n      <td>1.904552</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"#some more empty lists\ncountry_confirmed_cases = []\ncountry_death_cases = []\ncountry_active_cases = []\ncountry_recovery_cases = []\ncountry_mortality_rate = []\n\nno_cases = []\n\n#for loop to check in the unique countries if anyone has a confirmed covid case\nfor i in unique_countries:\n    cases = latest_data[latest_data['Country_Region']==i['Confirmed'].sum()]\n    if cases > 0:\n        country_confirmed_cases.appened(cases)\n    else:\n        no_cases.append(i)\n\n\n\n#for loop to remove a country if it has no covid\nfor i in no_cases:\n    unique_countries.remove(i)\n    \n#for loop to sort thru the countries and then add data to assorted lists    \nimport operator\n\nunique_countries = [k for k, v in sorted(zip(unique_countries, country_confirmed_cases), key=operator.itemgetter(1), reverse=True)]\nfor i in range(len(unique_countries)):\n    country_confirmed_cases[i] = latest_data[latest_data['Country_Region']==unique_countries[i]['Confirmed'].sum()]\n    country_death_cases.append(latest_data[latest_data['Country_Region']==unique_countries[i]]['Deaths'].sum())\n    country_recovery_cases.append(latest_data[latest_data['Country_Region']==unique_countries[i]]['Recovered'].sum())\n    country_active_cases.append(country_confirmed_cases[i] - country_death_cases[i] - country_recovery_cases[i])\n    country_mortality_rate.append(country_death_cases[i]/country_confirmed_cases[i])\n\n# some more empty lists\ncountry_confirmed_cases = []\ncountry_death_cases = []\ncountry_active_cases = []\ncountry_recovery_cases = []\ncountry_mortality_rate = []\n\nno_cases = []\n\n# for loop to check in the unique countries if anyone has a confirmed covid case\nfor country in unique_countries:\n    cases = latest_data[latest_data['Country_Region'] == country]\n    if not cases.empty:\n        country_confirmed_cases.append(cases)\n    else:\n        no_cases.append(country)\n\n# for loop to remove a country if it has no covid\nfor no_case in no_cases:\n    unique_countries.remove(no_case)\n\n# for loop to sort through the countries and then add data to assorted lists\nimport operator\n\nunique_countries = [k for k, v in sorted(zip(unique_countries, country_confirmed_cases), key=operator.itemgetter(1), reverse=True)]\n\nfor i in range(len(unique_countries)):\n    country_confirmed_cases[i] = latest_data[latest_data['Country_Region'] == unique_countries[i]]\n    country_death_cases.append(country_confirmed_cases[i]['Deaths'].sum())\n    country_recovery_cases.append(country_confirmed_cases[i]['Recovered'].sum())\n    country_active_cases.append(country_confirmed_cases[i]['Confirmed'].sum() - country_death_cases[i] - country_recovery_cases[i])\n    country_mortality_rate.append(country_death_cases[i] / country_confirmed_cases[i]['Confirmed'].sum())\"\"\"\n\n# some more empty lists\ncountry_confirmed_cases = []\ncountry_death_cases = []\ncountry_active_cases = []\ncountry_recovery_cases = []\ncountry_mortality_rate = []\n\nno_cases = []\n\n# for loop to check in the unique countries if anyone has a confirmed covid case\nfor country in unique_countries:\n    cases = latest_data[latest_data['Country_Region'] == country]\n    if not cases.empty:\n        country_confirmed_cases.append(cases)\n    else:\n        no_cases.append(country)\n\n# for loop to remove a country if it has no covid\nfor no_case in no_cases:\n    unique_countries.remove(no_case)\n\n# Calculate the total confirmed cases for each country\ntotal_confirmed_by_country = [cases['Confirmed'].sum() for cases in country_confirmed_cases]\n\n# Sort based on total confirmed cases\nsorted_countries_data = sorted(zip(unique_countries, country_confirmed_cases, total_confirmed_by_country),\n                               key=lambda x: x[2], reverse=True)\n\n# Unpack the sorted data\nunique_countries, country_confirmed_cases, total_confirmed_by_country = zip(*sorted_countries_data)\n\n# for loop to add data to assorted lists\nfor i in range(len(unique_countries)):\n    country_death_cases.append(country_confirmed_cases[i]['Deaths'].sum())\n    country_recovery_cases.append(country_confirmed_cases[i]['Recovered'].sum())\n    country_active_cases.append(country_confirmed_cases[i]['Confirmed'].sum() - country_death_cases[i] - country_recovery_cases[i])\n    country_mortality_rate.append(country_death_cases[i] / country_confirmed_cases[i]['Confirmed'].sum())\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-07T22:52:22.816792Z","iopub.execute_input":"2024-02-07T22:52:22.817160Z","iopub.status.idle":"2024-02-07T22:52:23.007682Z","shell.execute_reply.started":"2024-02-07T22:52:22.817134Z","shell.execute_reply":"2024-02-07T22:52:23.006722Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#here we are making our own dataframe from the cultivated data we have currently\n#easier to mangage but a little complicated since its not in a CSV yet, we can export it out later\ncountry_df = pd.DataFrame({'Country Name': unique_countries, 'Number of Confirmed Cases': country_confirmed_cases,\n                          'Number of Deaths': country_death_cases, 'Number of Recoveries': country_recovery_cases,\n                          'Mortality Rate': country_mortality_rate})\ncountry_df.style.background_gradient(cmap='Blues')\n\n#creating a list of only unique countries from the already culled list\nunique_provinces = list(latest_data['Province_State'].unique())\n\n#empty list for future use below\nprovince_confirmed_cases = []\nprovince_country = []\nprovince_death_cases = []\nprovince_recovery_cases = []\nprovince_mortality_cases = []\nno_cases = []\n\n\"\"\"for i in unique_provinces:\n    cases = latest_data[latest_data['Province_State'] == i]\n    if not cases.empty:\n        province_confirmed_cases.append(cases)\n    else:\n        no_cases.append(i)\"\"\"\n        \nfor country in unique_provinces:\n    cases = latest_data[latest_data['Province_State'] == country]\n    if not cases.empty:\n        province_confirmed_cases.append(cases)\n    else:\n        no_cases.append(cases)\n\n#I get the error here\nfor i in no_cases:\n    unique_provinces.remove(country)\n\n#unique_provinces = [k for k, v in sorted(zip(unique_provinces, province_confirmed_cases), key=operator.itemgetter(1),reverse=True)]\n\n# Sorting based on total confirmed cases in each province\nunique_provinces = [k for k, v in sorted(zip(unique_provinces, province_confirmed_cases), key=lambda x: x[1]['Confirmed'].sum(), reverse=True)]\n\n# Updating province_confirmed_cases accordingly\nprovince_confirmed_cases = [v for k, v in sorted(zip(unique_provinces, province_confirmed_cases), key=lambda x: x[1]['Confirmed'].sum(), reverse=True)]\n\"\"\"for country in range(len(unique_provinces)):\n    province_confirmed_cases[country] = latest_data[latest_data['Province_State']==unique_provinces[i]['Confirmed'].sum()]\n    province_country.append(latest_data[latest_data['Province_State']==unique_provinces[country]]['Country_Region'].unique()[0])\n    province_death_cases.append(latest_data[latest_data['Province_State']==unique_provinces[country]]['Deaths'].sum())\n    province_recovery_cases.append(latest_data[latest_data['Province_State']==unique_provinces[country]]['Recovered'].sum())\n    province_mortality_rate.append(province_death_cases[country]/province_confirmed_cases[country])\n    \n    for i in range(len(unique_provinces)):\n    province_confirmed_cases[i] = latest_data[latest_data['Province_State'] == unique_provinces[i]['Confirmed'].sum()]\n    province_country.append(latest_data[latest_data['Province_State'] == unique_provinces[i]]['Country_Region'].unique()[0])\n    province_death_cases.append(latest_data[latest_data['Province_State'] == unique_provinces[i]]['Deaths'].sum())\n    province_recovery_cases.append(latest_data[latest_data['Province_State'] == unique_provinces[i]]['Recovered'].sum())\n    province_mortality_rate.append(province_death_cases[i] / province_confirmed_cases[i]['Confirmed'].sum())\n    \n    \n    \n    \n    \n    for i in range(len(unique_provinces)):\n    province_confirmed_cases[i] = latest_data[latest_data['Province_State'] == unique_provinces[i]]  # Remove ['Confirmed'].sum()\n    province_country.append(latest_data[latest_data['Province_State'] == unique_provinces[i]]['Country_Region'].unique()[0])\n    province_death_cases.append(latest_data[latest_data['Province_State'] == unique_provinces[i]]['Deaths'].sum())\n    province_recovery_cases.append(latest_data[latest_data['Province_State'] == unique_provinces[i]]['Recovered'].sum())\n    province_mortality_cases.append(province_death_cases[i] / province_confirmed_cases[i]['Confirmed'].sum())\n    \"\"\"\n\n\n    \n\n    \nfor i in range(len(unique_provinces)):\n    province_data = latest_data[latest_data['Province_State'] == unique_provinces[i]]\n\n    if not province_data.empty:\n        province_confirmed_cases[i] = province_data\n        country_region_unique = province_data['Country_Region'].unique()\n\n        # Check if the 'Country_Region' column has at least one element\n        if len(country_region_unique) > 0:\n            province_country.append(country_region_unique[0])\n        else:\n            province_country.append(\"Unknown\")  # or any default value\n\n        province_death_cases.append(province_data['Deaths'].sum())\n        province_recovery_cases.append(province_data['Recovered'].sum())\n\n        # Avoid division by zero\n        total_confirmed = province_confirmed_cases[i]['Confirmed'].sum()\n        if total_confirmed > 0:\n            province_mortality_cases.append(province_death_cases[i] / total_confirmed)\n        else:\n            province_mortality_cases.append(0)  # or any default value\n    else:\n        # Handle the case when the province_data DataFrame is empty\n        province_country.append(\"Unknown\")\n        province_death_cases.append(0)\n        province_recovery_cases.append(0)\n        province_mortality_cases.append(0)\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-02-07T23:14:25.257765Z","iopub.execute_input":"2024-02-07T23:14:25.258113Z","iopub.status.idle":"2024-02-07T23:14:26.413421Z","shell.execute_reply.started":"2024-02-07T23:14:25.258085Z","shell.execute_reply":"2024-02-07T23:14:26.412391Z"},"trusted":true},"execution_count":36,"outputs":[]}]}